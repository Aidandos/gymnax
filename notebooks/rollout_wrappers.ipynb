{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    max_speed: 8,\n",
      "    max_torque: 2.0,\n",
      "    dt: 0.05,\n",
      "    g: 10.0,\n",
      "    m: 1.0,\n",
      "    l: 1.0,\n",
      "    max_steps_in_episode: 200,\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from flax import linen as nn\n",
    "\n",
    "import gymnax\n",
    "from gymnax.experimental.dojos import EvaluationDojo\n",
    "\n",
    "# 2D State Space, 3D Obs Space, 1D Action Space [Continuous - Torque]\n",
    "rng, reset, step, env_params = gymnax.make(\"Pendulum-v0\")\n",
    "print(env_params)\n",
    "\n",
    "num_steps = 200\n",
    "parallel_episodes = 10\n",
    "rng, rng_net, rng_episode, rng_reset = jax.random.split(rng, 4)\n",
    "rng_batch = jax.random.split(rng, parallel_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalEvaluationAgent():\n",
    "    \"\"\" A Minimal Wrapper for an evaluation agent. \"\"\"\n",
    "    def __init__(self, policy):\n",
    "        \"\"\" Init all key features of the agent. E.g. this may include:\n",
    "            - Policy function network forward function\n",
    "            - Exploitation schedule to use in evaluation\n",
    "            - Here: Deterministic Agent - but could also be stochastic!\n",
    "        \"\"\"\n",
    "        self.policy = policy\n",
    "\n",
    "    def actor_step(self, key, agent_params, obs, actor_state):\n",
    "        \"\"\" Policy forward pass + return action and new state. \"\"\"\n",
    "        action = self.policy(agent_params, obs)\n",
    "        return action, actor_state\n",
    "    \n",
    "    def init_actor_state(self, evaluate):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Plain JAX MLP Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy_mlp(rng_input, sizes, scale=1e-2):\n",
    "    \"\"\" Initialize the weights of all layers of a relu + linear layer \"\"\"\n",
    "    # Initialize a single layer with Gaussian weights - helper function\n",
    "    def initialize_layer(m, n, key, scale):\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        return (scale * jax.random.normal(w_key, (n, m)),\n",
    "                scale * jax.random.normal(b_key, (n,)))\n",
    "\n",
    "    keys = jax.random.split(rng_input, len(sizes)+1)\n",
    "    W1, b1 = initialize_layer(sizes[0], sizes[1],\n",
    "                              keys[0], scale)\n",
    "    W2, b2 = initialize_layer(sizes[1], sizes[2],\n",
    "                              keys[1], scale)\n",
    "    params = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return params\n",
    "\n",
    "\n",
    "def PolicyJAX(params, obs):\n",
    "    \"\"\" Compute forward pass and return action from deterministic policy \"\"\"\n",
    "    def relu_layer(W, b, x):\n",
    "        \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "        return jnp.maximum(0, (jnp.dot(W, x) + b))\n",
    "    # Simple single hidden layer MLP: Obs -> Hidden -> Action\n",
    "    activations = relu_layer(params[\"W1\"], params[\"b1\"], obs)\n",
    "    mean_policy = jnp.dot(params[\"W2\"], activations) + params[\"b2\"]\n",
    "    return mean_policy\n",
    "\n",
    "input_dim = 3\n",
    "policy_params = init_policy_mlp(rng_net, sizes=[input_dim, 16, 1])\n",
    "agent = MinimalEvaluationAgent(PolicyJAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': DeviceArray([[-0.00144986, -0.00069665, -0.00387892],\n",
      "             [ 0.01257899,  0.01681408, -0.01116779],\n",
      "             [-0.00631701,  0.01585951,  0.01501916],\n",
      "             [-0.00559352, -0.01859516,  0.00729775],\n",
      "             [ 0.01136356, -0.02308235,  0.00455822],\n",
      "             [ 0.00179699, -0.00209346, -0.00381558],\n",
      "             [-0.00845862, -0.01149663, -0.01519296],\n",
      "             [ 0.01073758, -0.0068458 , -0.01145768],\n",
      "             [ 0.00275306,  0.0001691 ,  0.0185804 ],\n",
      "             [ 0.00978198,  0.0066713 , -0.00369183],\n",
      "             [ 0.01377987, -0.00318499,  0.00491256],\n",
      "             [ 0.00328942,  0.0002241 , -0.00772289],\n",
      "             [ 0.00664143, -0.00185791,  0.0073858 ],\n",
      "             [ 0.00764628, -0.00054423, -0.00770936],\n",
      "             [ 0.00191605,  0.00185747, -0.00518309],\n",
      "             [-0.01456241, -0.00582085, -0.02329833]], dtype=float32), 'b1': DeviceArray([-0.00135108, -0.01499715,  0.00870826,  0.00763346,\n",
      "              0.01009146,  0.02091292,  0.01135801,  0.01992207,\n",
      "             -0.00122545, -0.00251963,  0.00958727,  0.01249821,\n",
      "             -0.00841087,  0.00605416,  0.00467295, -0.00415589],            dtype=float32), 'W2': DeviceArray([[-0.01073056, -0.00545973,  0.0054781 ,  0.00547318,\n",
      "              -0.00171657, -0.0100142 , -0.017693  ,  0.01554334,\n",
      "               0.00165178, -0.0040093 ,  0.00418818, -0.00680546,\n",
      "               0.01279292, -0.00069458,  0.01644365,  0.00164149]],            dtype=float32), 'b2': DeviceArray([-0.00115825], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "collector = EvaluationDojo(agent, step, reset, env_params)\n",
    "collector.init_dojo(policy_params)\n",
    "trace, reward = collector.steps_rollout(rng_episode, num_steps)\n",
    "traces, rewards = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 µs ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "697 µs ± 55.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit traces, rewards = collector.steps_rollout(rng_episode, num_steps)\n",
    "%timeit trace, reward = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku MLP Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_fct(x):\n",
    "    \"\"\" Standard MLP policy network.\"\"\"\n",
    "    mlp = hk.Sequential([\n",
    "      hk.Flatten(),\n",
    "      hk.Linear(16), jax.nn.relu,\n",
    "      hk.Linear(1),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "\n",
    "PolicyHaiku = hk.without_apply_rng(hk.transform(policy_fct))\n",
    "obs, state = reset(rng_net, env_params)\n",
    "policy_params = PolicyHaiku.init(rng_net, obs)\n",
    "agent = MinimalEvaluationAgent(PolicyHaiku.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = EvaluationDojo(agent, step, reset, env_params)\n",
    "collector.init_dojo(policy_params)\n",
    "trace, reward = collector.steps_rollout(rng_episode, num_steps)\n",
    "traces, rewards = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit traces, rewards = collector.steps_rollout(rng_episode, num_steps)\n",
    "%timeit traces, rewards = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax MLP Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyFLAX(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(16, name='fc1')(x)\n",
    "        x = nn.relu(x)\n",
    "        action = nn.Dense(1, name='fc2')(x)\n",
    "        return action\n",
    "\n",
    "obs, state = reset(rng_net, env_params)\n",
    "policy_params = PolicyFLAX().init(rng_net, obs)\n",
    "agent = MinimalEvaluationAgent(PolicyFLAX().apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = EvaluationDojo(agent, step, reset, env_params)\n",
    "collector.init_dojo(policy_params)\n",
    "trace, reward = collector.steps_rollout(rng_episode, num_steps)\n",
    "traces, rewards = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit traces, rewards = collector.steps_rollout(rng_episode, num_steps)\n",
    "%timeit trace, reward = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trax MLP Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trax = False\n",
    "if run_trax:\n",
    "    # Trax import takes forever!!!\n",
    "    # But in terms of runtime it is a lot faster than Haiku/Flax\n",
    "    import trax\n",
    "    from trax import layers as tl\n",
    "\n",
    "    # Problem with Trax: Takes input differently\n",
    "    # ---- Haiku, JAX, Flax model(params, input)\n",
    "    # ---- Trax model(input, params)\n",
    "    # Need helper function that re-routes inputs\n",
    "\n",
    "    def policy_fct():\n",
    "        model = tl.Serial(\n",
    "          tl.Dense(16),\n",
    "          tl.Relu(),\n",
    "          tl.Dense(1),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    policy = policy_fct()\n",
    "    policy_params, _ = policy.init(trax.shapes.signature(obs))\n",
    "    def PolicyTrax(params, obs):\n",
    "        \"\"\" Helper for correct mapping of policy params & input\"\"\"\n",
    "        return policy(obs, params)\n",
    "    agent = MinimalEvaluationAgent(PolicyTrax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_trax:\n",
    "    collector = EvaluationDojo(agent, step, reset, env_params)\n",
    "    collector.init_dojo(policy_params)\n",
    "    trace, reward = collector.steps_rollout(rng_episode, num_steps)\n",
    "    traces, rewards = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_trax:\n",
    "    %timeit traces, rewards = collector.steps_rollout(rng_episode, num_steps)\n",
    "    %timeit trace, reward = collector.batch_rollout(rng_batch, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer Tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnax.utils import init_buffer, push_buffer, sample_buffer\n",
    "from gymnax.dojos import InterleavedDojo\n",
    "from gymnax.agents import MinimalInterleavedAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dummy step transition & store it in buffer\n",
    "rng, reset, step, env_params = gymnax.make(\"Pendulum-v0\")\n",
    "rng, key_reset, key_step = jax.random.split(rng, 3)\n",
    "obs, state = reset(key_reset, env_params)\n",
    "action = jnp.array([1])\n",
    "next_obs, next_state, reward, done, _ = step(key_step, env_params,\n",
    "                                             state, action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity = 11\n",
    "env_params[\"max_steps_in_episode\"] = 5\n",
    "# Initialize buffer with templates\n",
    "buffer = init_buffer(state, obs, action, capacity)\n",
    "step_experience = {\"state\": state,\n",
    "                   \"next_state\": next_state,\n",
    "                   \"obs\": obs,\n",
    "                   \"next_obs\": next_obs,\n",
    "                   \"action\": action,\n",
    "                   \"reward\": reward,\n",
    "                   \"done\": done}\n",
    "for i in range(11):\n",
    "    next_obs, next_state, reward, done, _ = step(key_step, env_params,\n",
    "                                                 state, action)\n",
    "    step_experience = {\"state\": state,\n",
    "                       \"next_state\": next_state,\n",
    "                       \"obs\": obs,\n",
    "                       \"next_obs\": next_obs,\n",
    "                       \"action\": action,\n",
    "                       \"reward\": reward,\n",
    "                       \"done\": done}\n",
    "    buffer = push_buffer(buffer, step_experience)\n",
    "    # Auto-reset environment and use obs/state if episode terminated\n",
    "    obs_reset, state_reset = reset(rng_reset, env_params)\n",
    "    next_obs = done * obs_reset + (1 - done) * next_obs\n",
    "    next_state = done * state_reset + (1 - done) * next_state\n",
    "    # Update state/obs for next step\n",
    "    state = next_state\n",
    "    obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key_sample = jax.random.split(rng)\n",
    "sample_buffer(key_sample, buffer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MinimalInterleavedAgent(PolicyJAX)\n",
    "collector = InterleavedDojo(agent, buffer, push_buffer, sample_buffer,\n",
    "                            step, reset, env_params)\n",
    "collector.init_dojo(policy_params)\n",
    "trace, reward = collector.steps_rollout(rng_episode, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def zipdir(path: str, zip_fname: str):\n",
    "    \"\"\" Zip a directory to upload afterwards to GCloud Storage. \"\"\"\n",
    "    # ziph is zipfile handle\n",
    "    ziph = zipfile.ZipFile(zip_fname, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "    ziph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path, file = os.path.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/rtl/Dropbox/core-code/mle-toolbox/examples\"\n",
    "prefix_len = len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(path):\n",
    "    print(root, dirs, files)\n",
    "    print(os.path.join(root[prefix_len+1:], file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ma-vision)",
   "language": "python",
   "name": "ma-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
