{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from haiku import nets\n",
    "import optax\n",
    "import rlax\n",
    "import collections\n",
    "\n",
    "import gymnax\n",
    "from gymnax.dojos import InterleavedDojo\n",
    "from gymnax.utils import init_buffer, push_buffer, sample_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import gymnax catch bsuite environment + Init replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_steps_in_episode': 2000}\n"
     ]
    }
   ],
   "source": [
    "rng, reset, step, env_params = gymnax.make(\"Catch-bsuite\")\n",
    "print(env_params)\n",
    "rng, key_reset, key_step = jax.random.split(rng, 3)\n",
    "obs, state = reset(key_reset, env_params)\n",
    "action = jnp.array([0])\n",
    "\n",
    "capacity = 2000\n",
    "buffer = init_buffer(state, obs, action, capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = collections.namedtuple(\"Params\", \"online target\")\n",
    "ActorState = collections.namedtuple(\"ActorState\", \"count evaluation\")\n",
    "ActorOutput = collections.namedtuple(\"ActorOutput\", \"actions q_values\")\n",
    "LearnerState = collections.namedtuple(\"LearnerState\", \"count opt_state\")\n",
    "\n",
    "\n",
    "def build_network(num_actions: int) -> hk.Transformed:\n",
    "    \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
    "    def q(obs):\n",
    "        network = hk.Sequential(\n",
    "            [hk.Flatten(),\n",
    "             nets.MLP([50, num_actions])])\n",
    "        return network(obs)\n",
    "    return hk.without_apply_rng(hk.transform(q, apply_rng=True))\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    \"\"\"A simple DQN agent.\"\"\"\n",
    "    def __init__(self, obs_template, num_actions, epsilon_cfg,\n",
    "                 target_period, learning_rate):\n",
    "        self._obs_template = obs_template\n",
    "        self._num_actions = num_actions\n",
    "        self._target_period = target_period\n",
    "        # Neural net and optimiser.\n",
    "        self._network = build_network(num_actions)\n",
    "        self._optimizer = optax.adam(learning_rate)\n",
    "        self._epsilon_by_frame = optax.polynomial_schedule(**epsilon_cfg)\n",
    "\n",
    "    def initial_params(self, key):\n",
    "        sample_input = jnp.expand_dims(self._obs_template, 0)\n",
    "        online_params = self._network.init(key, sample_input)\n",
    "        return Params(online_params, online_params)\n",
    "\n",
    "    def init_actor_state(self):\n",
    "        actor_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        return ActorState(actor_count, bool(0))\n",
    "\n",
    "    def init_learner_state(self, params):\n",
    "        learner_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        opt_state = self._optimizer.init(params.online)\n",
    "        return LearnerState(learner_count, opt_state)\n",
    "\n",
    "    def actor_step(self, key, params, obs, actor_state):\n",
    "        obs = jnp.expand_dims(obs, 0)  # add dummy batch\n",
    "        q = self._network.apply(params.online, obs)[0]    # remove dummy batch\n",
    "        epsilon = self._epsilon_by_frame(actor_state.count)\n",
    "        train_a = rlax.epsilon_greedy(epsilon).sample(key, q)\n",
    "        eval_a = rlax.greedy().sample(key, q)\n",
    "        a = jax.lax.select(actor_state.evaluation, eval_a, train_a)\n",
    "        return (a, ActorState(actor_state.count + 1, bool(0)))\n",
    "\n",
    "    def learner_step(self, key, params, learner_state, data):\n",
    "        target_params = rlax.periodic_update(\n",
    "            params.online, params.target,\n",
    "            learner_state.count, self._target_period)\n",
    "        dloss_dtheta = jax.grad(self._loss)(params.online,\n",
    "                                            target_params,\n",
    "                                           data[\"obs\"], data[\"action\"],\n",
    "                                           data[\"reward\"], data[\"done\"],\n",
    "                                           data[\"next_obs\"])\n",
    "        updates, opt_state = self._optimizer.update(dloss_dtheta,\n",
    "                                                    learner_state.opt_state)\n",
    "        online_params = optax.apply_updates(params.online, updates)\n",
    "        return (Params(online_params, target_params),\n",
    "                LearnerState(learner_state.count + 1, opt_state))\n",
    "\n",
    "    def _loss(self, online_params, target_params,\n",
    "              obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "        q_tm1 = self._network.apply(online_params, obs_tm1)\n",
    "        q_t_val = self._network.apply(target_params, obs_t)\n",
    "        q_t_select = self._network.apply(online_params, obs_t)\n",
    "        batched_loss = jax.vmap(rlax.double_q_learning)\n",
    "        # TODO: Problems with chex in rlax function!\n",
    "        # Rank compatibility = squeeze inputs\n",
    "        # Type compatibility = make actions of type int\n",
    "        td_error = batched_loss(q_tm1, a_tm1.squeeze().astype(int), r_t.squeeze(),\n",
    "                                discount_t.squeeze(), q_t_val, q_t_select)\n",
    "        return jnp.mean(rlax.l2_loss(td_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Rollout/Learning Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3\n",
    "epsilon_cfg = dict(init_value=1,\n",
    "                   end_value=0.01,\n",
    "                   transition_steps=1000,\n",
    "                   power=1.)\n",
    "target_period = 50\n",
    "learning_rate = 0.005\n",
    "\n",
    "rng, rng_net, rng_episode = jax.random.split(rng, 3)\n",
    "agent = DQN(obs, num_actions, epsilon_cfg,\n",
    "            target_period, learning_rate)\n",
    "agent_params = agent.initial_params(rng_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = InterleavedDojo(agent, buffer,\n",
    "                            push_buffer, sample_buffer,\n",
    "                            step, reset, env_params)\n",
    "collector.init_dojo(agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace, reward = collector.steps_rollout(rng_episode, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorState(count=DeviceArray(2000., dtype=float32), evaluation=DeviceArray(False, dtype=bool))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.10314138,  0.01692898,  0.00339818, -0.22555402,\n",
       "             -0.2493881 , -0.23812832, -0.20821935, -0.00669516,\n",
       "             -0.08488692, -0.17770854, -0.1558158 , -0.04922869,\n",
       "             -0.10872687, -0.1756797 ,  0.04488778, -0.09596258,\n",
       "             -0.03472243, -0.11578089,  0.0248939 , -0.1198677 ,\n",
       "             -0.00902543, -0.08586885, -0.27267307, -0.16214508,\n",
       "             -0.25532457, -0.13796015, -0.03581421,  0.00128941,\n",
       "             -0.04938221, -0.07835183,  0.04129767, -0.20477088,\n",
       "             -0.02650824, -0.19110182,  0.01638899, -0.07326864,\n",
       "              0.03354215, -0.14502908, -0.15484948, -0.21393028,\n",
       "              0.03954035, -0.05847014, -0.15489167, -0.19154248,\n",
       "             -0.09403924, -0.03897392, -0.09241311,  0.07896563,\n",
       "             -0.16529782, -0.18596666], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rng, obs, state, env_params, agent_params, actor_state, learner_state\n",
    "trace[4].online['mlp/~/linear_0']['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.10314138,  0.02108564,  0.00517962, -0.22555402,\n",
       "             -0.24818853, -0.23812832, -0.20821935, -0.02015002,\n",
       "             -0.08994701, -0.17767121, -0.15541175, -0.0492282 ,\n",
       "             -0.10871557, -0.17570056,  0.04945984, -0.12474462,\n",
       "             -0.03413429, -0.11578089,  0.03364721, -0.12034842,\n",
       "             -0.0093787 , -0.0923802 , -0.272653  , -0.16214508,\n",
       "             -0.2555447 , -0.13796015, -0.03337045,  0.00072547,\n",
       "             -0.04913528, -0.08143973,  0.04173509, -0.2047213 ,\n",
       "             -0.02951946, -0.19110125,  0.00712014, -0.10195624,\n",
       "              0.03304003, -0.14502908, -0.15505567, -0.21393028,\n",
       "              0.04468149, -0.08397934, -0.17972314, -0.19129492,\n",
       "             -0.12606324, -0.06173234, -0.09620329,  0.08100923,\n",
       "             -0.16520287, -0.18596666], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace[4].target['mlp/~/linear_0']['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ma-vision)",
   "language": "python",
   "name": "ma-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
