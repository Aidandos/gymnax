{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from haiku import nets\n",
    "import optax\n",
    "import rlax\n",
    "\n",
    "import gymnax\n",
    "from gymnax.rollouts import InterleavedRollouts\n",
    "from gymnax.rollouts import init_buffer, push_buffer, sample_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import gymnax catch bsuite environment + Init replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, reset, step, env_params = gymnax.make(\"Catch-bsuite\")\n",
    "rng, key_reset, key_step = jax.random.split(rng, 3)\n",
    "obs, state = reset(key_reset, env_params)\n",
    "action = jnp.array([0])\n",
    "\n",
    "capacity = 2000\n",
    "buffer = init_buffer(state, obs, action, capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"A simple DQN agent.\"\"\"\n",
    "    def __init__(self, obs_template, num_actions, epsilon_cfg,\n",
    "                 target_period, learning_rate):\n",
    "        self._observation_template = observation_template\n",
    "        self._action_template = action_template\n",
    "        self._target_period = target_period\n",
    "        # Neural net and optimiser.\n",
    "        self._network = build_network(num_actions)\n",
    "        self._optimizer = optax.adam(learning_rate)\n",
    "        self._epsilon_by_frame = optax.polynomial_schedule(**epsilon_cfg)\n",
    "\n",
    "    def initial_params(self, key):\n",
    "        sample_input = jnp.expand_dims(self._observation_template, 0)\n",
    "        online_params = self._network.init(key, sample_input)\n",
    "        return Params(online_params, online_params)\n",
    "\n",
    "    def init_actor_state(self):\n",
    "        actor_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        return ActorState(actor_count)\n",
    "\n",
    "    def init_learner_state(self, params):\n",
    "        learner_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        opt_state = self._optimizer.init(params.online)\n",
    "        return LearnerState(learner_count, opt_state)\n",
    "\n",
    "    def actor_step(self, key, params, obs, actor_statexw):\n",
    "        obs = jnp.expand_dims(env_output.observation, 0)  # add dummy batch\n",
    "        q = self._network.apply(params.online, obs)[0]    # remove dummy batch\n",
    "        epsilon = self._epsilon_by_frame(actor_state.count)\n",
    "        train_a = rlax.epsilon_greedy(epsilon).sample(key, q)\n",
    "        eval_a = rlax.greedy().sample(key, q)\n",
    "        a = jax.lax.select(actor_state.evaluation, eval_a, train_a)\n",
    "        return (ActorOutput(actions=a, q_values=q),\n",
    "                ActorState(actor_state.count + 1))\n",
    "\n",
    "    def learner_step(self, params, learner_state, data):\n",
    "        target_params = rlax.periodic_update(\n",
    "            params.online, params.target,\n",
    "            learner_state.count, self._target_period)\n",
    "        dloss_dtheta = jax.grad(self._loss)(params.online,\n",
    "                                            target_params, *data)\n",
    "        updates, opt_state = self._optimizer.update(dloss_dtheta,\n",
    "                                                    learner_state.opt_state)\n",
    "        online_params = optax.apply_updates(params.online, updates)\n",
    "        return (Params(online_params, target_params),\n",
    "                LearnerState(learner_state.count + 1, opt_state))\n",
    "\n",
    "    def _loss(self, online_params, target_params,\n",
    "            obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "        q_tm1 = self._network.apply(online_params, obs_tm1)\n",
    "        q_t_val = self._network.apply(target_params, obs_t)\n",
    "        q_t_select = self._network.apply(online_params, obs_t)\n",
    "        batched_loss = jax.vmap(rlax.double_q_learning)\n",
    "        td_error = batched_loss(q_tm1, a_tm1, r_t, discount_t,\n",
    "                                q_t_val, q_t_select)\n",
    "        return jnp.mean(rlax.l2_loss(td_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Rollout/Learning Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = InterleavedRollouts(agent, buffer,\n",
    "                                push_buffer, sample_buffer,\n",
    "                                step, reset, env_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ma-vision)",
   "language": "python",
   "name": "ma-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
