{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from haiku import nets\n",
    "import optax\n",
    "import rlax\n",
    "import collections\n",
    "\n",
    "import gymnax\n",
    "from gymnax.dojos import InterleavedDojo\n",
    "from gymnax.utils import init_buffer, push_buffer, sample_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import gymnax catch bsuite environment + Init replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_steps_in_episode': 2000}\n"
     ]
    }
   ],
   "source": [
    "rng, reset, step, env_params = gymnax.make(\"Catch-bsuite\")\n",
    "print(env_params)\n",
    "rng, key_reset, key_step = jax.random.split(rng, 3)\n",
    "obs, state = reset(key_reset, env_params)\n",
    "action = jnp.array([0])\n",
    "\n",
    "capacity = 2000\n",
    "buffer = init_buffer(state, obs, action, capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = collections.namedtuple(\"Params\", \"online target\")\n",
    "ActorState = collections.namedtuple(\"ActorState\", \"count evaluation\")\n",
    "ActorOutput = collections.namedtuple(\"ActorOutput\", \"actions q_values\")\n",
    "LearnerState = collections.namedtuple(\"LearnerState\", \"count opt_state\")\n",
    "\n",
    "\n",
    "def build_network(num_actions: int) -> hk.Transformed:\n",
    "    \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
    "    def q(obs):\n",
    "        network = hk.Sequential(\n",
    "            [hk.Flatten(),\n",
    "             nets.MLP([50, num_actions])])\n",
    "        return network(obs)\n",
    "    return hk.without_apply_rng(hk.transform(q, apply_rng=True))\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    \"\"\"A simple DQN agent.\"\"\"\n",
    "    def __init__(self, obs_template, num_actions, epsilon_cfg,\n",
    "                 target_period, learning_rate):\n",
    "        self._obs_template = obs_template\n",
    "        self._num_actions = num_actions\n",
    "        self._target_period = target_period\n",
    "        # Neural net and optimiser.\n",
    "        self._network = build_network(num_actions)\n",
    "        self._optimizer = optax.adam(learning_rate)\n",
    "        self._epsilon_by_frame = optax.polynomial_schedule(**epsilon_cfg)\n",
    "\n",
    "    def initial_params(self, key):\n",
    "        sample_input = jnp.expand_dims(self._obs_template, 0)\n",
    "        online_params = self._network.init(key, sample_input)\n",
    "        return Params(online_params, online_params)\n",
    "\n",
    "    def init_actor_state(self):\n",
    "        actor_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        return ActorState(actor_count, bool(0))\n",
    "\n",
    "    def init_learner_state(self, params):\n",
    "        learner_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        opt_state = self._optimizer.init(params.online)\n",
    "        return LearnerState(learner_count, opt_state)\n",
    "\n",
    "    def actor_step(self, key, params, obs, actor_state):\n",
    "        obs = jnp.expand_dims(obs, 0)  # add dummy batch\n",
    "        q = self._network.apply(params.online, obs)[0]    # remove dummy batch\n",
    "        epsilon = self._epsilon_by_frame(actor_state.count)\n",
    "        train_a = rlax.epsilon_greedy(epsilon).sample(key, q)\n",
    "        eval_a = rlax.greedy().sample(key, q)\n",
    "        a = jax.lax.select(actor_state.evaluation, eval_a, train_a)\n",
    "        return (a, ActorState(actor_state.count + 1, bool(0)))\n",
    "\n",
    "    def learner_step(self, key, params, learner_state, data):\n",
    "        target_params = rlax.periodic_update(\n",
    "            params.online, params.target,\n",
    "            learner_state.count, self._target_period)\n",
    "        dloss_dtheta = jax.grad(self._loss)(params.online,\n",
    "                                            target_params,\n",
    "                                           data[\"obs\"], data[\"action\"],\n",
    "                                           data[\"reward\"], data[\"done\"],\n",
    "                                           data[\"next_obs\"])\n",
    "        updates, opt_state = self._optimizer.update(dloss_dtheta,\n",
    "                                                    learner_state.opt_state)\n",
    "        online_params = optax.apply_updates(params.online, updates)\n",
    "        return (Params(online_params, target_params),\n",
    "                LearnerState(learner_state.count + 1, opt_state))\n",
    "\n",
    "    def _loss(self, online_params, target_params,\n",
    "              obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "        q_tm1 = self._network.apply(online_params, obs_tm1)\n",
    "        q_t_val = self._network.apply(target_params, obs_t)\n",
    "        q_t_select = self._network.apply(online_params, obs_t)\n",
    "        batched_loss = jax.vmap(rlax.double_q_learning)\n",
    "        # TODO: Problems with chex in rlax function!\n",
    "        # Rank compatibility = squeeze inputs\n",
    "        # Type compatibility = make actions of type int\n",
    "        td_error = batched_loss(q_tm1, a_tm1.squeeze().astype(int), r_t.squeeze(),\n",
    "                                discount_t.squeeze(), q_t_val, q_t_select)\n",
    "        return jnp.mean(rlax.l2_loss(td_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Rollout/Learning Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 3\n",
    "epsilon_cfg = dict(init_value=1,\n",
    "                   end_value=0.01,\n",
    "                   transition_steps=1000,\n",
    "                   power=1.)\n",
    "target_period = 50\n",
    "learning_rate = 0.005\n",
    "\n",
    "parallel_episodes = 5\n",
    "rng, rng_net, rng_episode = jax.random.split(rng, 3)\n",
    "rng_batch = jax.random.split(rng, parallel_episodes)\n",
    "agent = DQN(obs, num_actions, epsilon_cfg,\n",
    "            target_period, learning_rate)\n",
    "agent_params = agent.initial_params(rng_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = InterleavedDojo(agent, buffer,\n",
    "                            push_buffer, sample_buffer,\n",
    "                            step, reset, env_params)\n",
    "collector.init_dojo(agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace, reward = collector.episode_rollout(rng_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ma-vision)",
   "language": "python",
   "name": "ma-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
