{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "from haiku import nets\n",
    "import optax\n",
    "import rlax\n",
    "import collections\n",
    "\n",
    "import gymnax\n",
    "from gymnax.dojos import InterleavedDojo, EvaluationDojo\n",
    "from gymnax.utils import init_buffer, push_buffer, sample_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import gymnax catch bsuite environment + Init replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_steps_in_episode': 2000}\n"
     ]
    }
   ],
   "source": [
    "rng, reset, step, env_params = gymnax.make(\"Catch-bsuite\")\n",
    "print(env_params)\n",
    "rng, key_reset, key_step = jax.random.split(rng, 3)\n",
    "obs, state = reset(key_reset, env_params)\n",
    "action = jnp.array([0])\n",
    "\n",
    "capacity = 2000\n",
    "buffer = init_buffer(state, obs, action, capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = collections.namedtuple(\"Params\", \"online target\")\n",
    "ActorState = collections.namedtuple(\"ActorState\", \"count evaluation\")\n",
    "ActorOutput = collections.namedtuple(\"ActorOutput\", \"actions q_values\")\n",
    "LearnerState = collections.namedtuple(\"LearnerState\", \"count opt_state\")\n",
    "\n",
    "\n",
    "def build_network(num_actions: int) -> hk.Transformed:\n",
    "    \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
    "    def q(obs):\n",
    "        network = hk.Sequential(\n",
    "            [hk.Flatten(),\n",
    "             nets.MLP([50, num_actions])])\n",
    "        return network(obs)\n",
    "    return hk.without_apply_rng(hk.transform(q, apply_rng=True))\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    \"\"\"A simple DQN agent.\"\"\"\n",
    "    def __init__(self, obs_template, num_actions, epsilon_cfg,\n",
    "                 target_period, learning_rate):\n",
    "        self._obs_template = obs_template\n",
    "        self._num_actions = num_actions\n",
    "        self._target_period = target_period\n",
    "        # Neural net and optimiser.\n",
    "        self._network = build_network(num_actions)\n",
    "        self._optimizer = optax.adam(learning_rate)\n",
    "        self._epsilon_by_frame = optax.polynomial_schedule(**epsilon_cfg)\n",
    "\n",
    "    def initial_params(self, key):\n",
    "        sample_input = jnp.expand_dims(self._obs_template, 0)\n",
    "        online_params = self._network.init(key, sample_input)\n",
    "        return Params(online_params, online_params)\n",
    "\n",
    "    def init_actor_state(self, evaluate=False):\n",
    "        actor_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        return ActorState(actor_count, evaluate)\n",
    "\n",
    "    def init_learner_state(self, params):\n",
    "        learner_count = jnp.zeros((), dtype=jnp.float32)\n",
    "        opt_state = self._optimizer.init(params.online)\n",
    "        return LearnerState(learner_count, opt_state)\n",
    "\n",
    "    def actor_step(self, key, params, obs, actor_state):\n",
    "        obs = jnp.expand_dims(obs, 0)  # add dummy batch\n",
    "        q = self._network.apply(params.online, obs)[0]    # remove dummy batch\n",
    "        epsilon = self._epsilon_by_frame(actor_state.count)\n",
    "        train_a = rlax.epsilon_greedy(epsilon).sample(key, q)\n",
    "        eval_a = rlax.greedy().sample(key, q)\n",
    "        a = jax.lax.select(actor_state.evaluation, eval_a, train_a)\n",
    "        return (a, ActorState(actor_state.count + 1, bool(0)))\n",
    "\n",
    "    def learner_step(self, key, params, learner_state, data):\n",
    "        target_params = rlax.periodic_update(\n",
    "            params.online, params.target,\n",
    "            learner_state.count, self._target_period)\n",
    "        dloss_dtheta = jax.grad(self._loss)(params.online,\n",
    "                                            target_params,\n",
    "                                           data[\"obs\"], data[\"action\"],\n",
    "                                           data[\"reward\"], data[\"done\"],\n",
    "                                           data[\"next_obs\"])\n",
    "        updates, opt_state = self._optimizer.update(dloss_dtheta,\n",
    "                                                    learner_state.opt_state)\n",
    "        online_params = optax.apply_updates(params.online, updates)\n",
    "        return (Params(online_params, target_params),\n",
    "                LearnerState(learner_state.count + 1, opt_state))\n",
    "\n",
    "    def _loss(self, online_params, target_params,\n",
    "              obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
    "        q_tm1 = self._network.apply(online_params, obs_tm1)\n",
    "        q_t_val = self._network.apply(target_params, obs_t)\n",
    "        q_t_select = self._network.apply(online_params, obs_t)\n",
    "        batched_loss = jax.vmap(rlax.double_q_learning)\n",
    "        # TODO: Problems with chex in rlax function!\n",
    "        # Rank compatibility = squeeze inputs\n",
    "        # Type compatibility = make actions of type int\n",
    "        td_error = batched_loss(q_tm1, a_tm1.squeeze().astype(int), r_t.squeeze(),\n",
    "                                discount_t.squeeze(), q_t_val, q_t_select)\n",
    "        return jnp.mean(rlax.l2_loss(td_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Rollout/Learning Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "num_actions = 3\n",
    "epsilon_cfg = dict(init_value=1,\n",
    "                   end_value=0.01,\n",
    "                   transition_steps=1000,\n",
    "                   power=1.)\n",
    "target_period = 50\n",
    "learning_rate = 0.005\n",
    "num_steps = 2000\n",
    "\n",
    "rng, rng_net, rng_episode = jax.random.split(rng, 3)\n",
    "agent = DQN(obs, num_actions, epsilon_cfg,\n",
    "            target_period, learning_rate)\n",
    "agent_params = agent.initial_params(rng_net)\n",
    "print(agent_params.online['mlp/~/linear_0']['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = InterleavedDojo(agent, buffer,\n",
    "                            push_buffer, sample_buffer,\n",
    "                            step, reset, env_params)\n",
    "collector.init_dojo(agent_params)\n",
    "\n",
    "evaluator = EvaluationDojo(agent, step, reset, env_params)\n",
    "evaluator.init_dojo()\n",
    "\n",
    "trace, reward = collector.steps_rollout(rng_episode, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_evals = jax.random.split(rng, 10)\n",
    "_, reward = evaluator.batch_rollout(rng_evals, 9,\n",
    "                                    collector.agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.8, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sum(reward, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.05110177, -0.07870402, -0.20903331,  0.00914063,\n",
       "             -0.12843364, -0.2133825 , -0.3431929 , -0.17656963,\n",
       "             -0.03682008, -0.15827283, -0.16298941, -0.14577693,\n",
       "             -0.11531142, -0.03487734, -0.09916329, -0.09522908,\n",
       "             -0.23822062, -0.1428581 ,  0.0028815 , -0.13443884,\n",
       "             -0.10411434, -0.15547672, -0.20288026, -0.08027034,\n",
       "             -0.1311672 , -0.09274957, -0.17738967,  0.00871049,\n",
       "             -0.13293147, -0.18503276, -0.2511608 , -0.01409638,\n",
       "             -0.02259834, -0.15740563, -0.13881627, -0.23443146,\n",
       "             -0.12356479, -0.20620756, -0.15213154, -0.18423879,\n",
       "             -0.06680408, -0.26913723, -0.2541214 , -0.06354713,\n",
       "             -0.07263365, -0.25095072, -0.12604818, -0.2254671 ,\n",
       "             -0.02294958, -0.22215189], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rng, obs, state, env_params, agent_params, actor_state, learner_state\n",
    "trace[4].online['mlp/~/linear_0']['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.05408095, -0.07936773, -0.20892192,  0.00642701,\n",
       "             -0.12937585, -0.2133825 , -0.3431929 , -0.17696036,\n",
       "             -0.03570051, -0.15827283, -0.16146871, -0.14700289,\n",
       "             -0.11531142, -0.03505527, -0.0974686 , -0.09397108,\n",
       "             -0.23811772, -0.14034916,  0.00101197, -0.13442558,\n",
       "             -0.10348068, -0.15548068, -0.20295212, -0.08296234,\n",
       "             -0.13158248, -0.08579965, -0.18577208,  0.00947669,\n",
       "             -0.13223554, -0.18503276, -0.24886422, -0.01140631,\n",
       "             -0.02271254, -0.15776223, -0.134745  , -0.23800932,\n",
       "             -0.12370464, -0.20493427, -0.15213154, -0.18423711,\n",
       "             -0.06602716, -0.27030075, -0.2541113 , -0.06344025,\n",
       "             -0.07198612, -0.25095072, -0.12604818, -0.22617814,\n",
       "             -0.02164434, -0.2215555 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace[4].target['mlp/~/linear_0']['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.05110177, -0.07870402, -0.20903331,  0.00914063,\n",
       "             -0.12843364, -0.2133825 , -0.3431929 , -0.17656963,\n",
       "             -0.03682008, -0.15827283, -0.16298941, -0.14577693,\n",
       "             -0.11531142, -0.03487734, -0.09916329, -0.09522908,\n",
       "             -0.23822062, -0.1428581 ,  0.0028815 , -0.13443884,\n",
       "             -0.10411434, -0.15547672, -0.20288026, -0.08027034,\n",
       "             -0.1311672 , -0.09274957, -0.17738967,  0.00871049,\n",
       "             -0.13293147, -0.18503276, -0.2511608 , -0.01409638,\n",
       "             -0.02259834, -0.15740563, -0.13881627, -0.23443146,\n",
       "             -0.12356479, -0.20620756, -0.15213154, -0.18423879,\n",
       "             -0.06680408, -0.26913723, -0.2541214 , -0.06354713,\n",
       "             -0.07263365, -0.25095072, -0.12604818, -0.2254671 ,\n",
       "             -0.02294958, -0.22215189], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector.agent_params.online['mlp/~/linear_0']['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_conda = (\"source $(conda info --base)/etc/profile.d/conda.sh \"\n",
    "                \"&& conda activate {remote_env_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'source $(conda info --base)/etc/profile.d/conda.sh && conda activate {remote_env_name}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ma-vision)",
   "language": "python",
   "name": "ma-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
