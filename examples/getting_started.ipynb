{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gymnax`: Classic Gym Environments in JAX\n",
    "### [Last Update: July 2022][![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RobertTLange/gymnax/blob/main/examples/getting_started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic API: `gymnax.make()`, `env.reset()`, `env.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import gymnax\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_policy, key_step = jax.random.split(rng, 4)\n",
    "\n",
    "env, env_params = gymnax.make(\"Pendulum-v1\")\n",
    "\n",
    "obs, state = env.reset(key_reset, env_params)\n",
    "action = env.action_space(env_params).sample(key_policy)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add simple vmap for step/reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jitted Episode Rollouts via `lax.scan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple ReLU MLP.\"\"\"\n",
    "\n",
    "    num_hidden_units: int\n",
    "    num_hidden_layers: int\n",
    "    num_output_units: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, rng):\n",
    "        for l in range(self.num_hidden_layers):\n",
    "            x = nn.Dense(features=self.num_hidden_units)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.num_output_units)(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "network = MLP(48, 1, 1)\n",
    "policy_params = network.init(rng, jnp.zeros(3), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(rng_input, policy_params, env_params, num_env_steps):\n",
    "    \"\"\"Rollout a jitted gymnax episode with lax.scan.\"\"\"\n",
    "    # Reset the environment\n",
    "    rng_reset, rng_episode = jax.random.split(rng_input)\n",
    "    obs, state = env.reset(rng_reset, env_params)\n",
    "\n",
    "    def policy_step(state_input, tmp):\n",
    "        \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "        obs, state, policy_params, rng = state_input\n",
    "        rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "        action = network.apply(policy_params, obs, rng_net)\n",
    "        next_o, next_s, reward, done, _ = env.step(\n",
    "          rng_step, state, action, env_params\n",
    "        )\n",
    "        carry = [next_o.squeeze(), next_s, policy_params, rng]\n",
    "        return carry, [next_o.squeeze(), reward, done]\n",
    "\n",
    "    # Scan over episode step loop\n",
    "    _, scan_out = jax.lax.scan(\n",
    "      policy_step,\n",
    "      [obs, state, policy_params, rng_episode],\n",
    "      [jnp.zeros((num_env_steps, 3 + 1 + 1))],\n",
    "    )\n",
    "    # Return masked sum of rewards accumulated by agent in episode\n",
    "    obs, rewards, dones = scan_out[0], scan_out[1], scan_out[2]\n",
    "    rewards = rewards.reshape(num_env_steps, 1)\n",
    "    ep_mask = (jnp.cumsum(dones) < 1).reshape(num_env_steps, 1)\n",
    "    return obs, rewards, dones, jnp.sum(rewards * ep_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jit-Compiled Episode Rollout\n",
    "jit_rollout = jax.jit(rollout, static_argnums=3)\n",
    "all_obs, all_rewards, all_dones, returns = jit_rollout(rng, policy_params, env_params, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[[ 0.9998754 , -0.01578528, -0.60411924],\n",
       "               [ 0.9991462 , -0.04131402, -0.5107971 ],\n",
       "               [ 0.9980124 , -0.06301779, -0.4346757 ],\n",
       "               ...,\n",
       "               [ 0.47610185,  0.8793902 , -6.0634437 ],\n",
       "               [ 0.6958211 ,  0.71821517, -5.466909  ],\n",
       "               [ 0.8506419 ,  0.5257455 , -4.9528723 ]],\n",
       " \n",
       "              [[ 0.75825185, -0.6519618 , -0.65798163],\n",
       "               [ 0.72351956, -0.69030386, -1.0348029 ],\n",
       "               [ 0.6717505 , -0.74077755, -1.4463619 ],\n",
       "               ...,\n",
       "               [-0.9916202 , -0.12918745, -8.        ],\n",
       "               [-0.9636502 ,  0.26716706, -8.        ],\n",
       "               [-0.78353995,  0.62134147, -8.        ]],\n",
       " \n",
       "              [[-0.55886924,  0.8292558 , -0.06539035],\n",
       "               [-0.57996494,  0.8146415 ,  0.5132828 ],\n",
       "               [-0.6237751 ,  0.7816039 ,  1.0975566 ],\n",
       "               ...,\n",
       "               [-0.26864362, -0.96323967,  7.5449786 ],\n",
       "               [ 0.08350886, -0.99650705,  7.111786  ],\n",
       "               [ 0.40486178, -0.9143779 ,  6.664406  ]],\n",
       " \n",
       "              ...,\n",
       " \n",
       "              [[-0.3342463 ,  0.94248575,  0.43098563],\n",
       "               [-0.38688007,  0.92213005,  1.1288075 ],\n",
       "               [-0.4696429 ,  0.8828565 ,  1.8328114 ],\n",
       "               ...,\n",
       "               [ 0.02051732, -0.9997895 ,  7.198631  ],\n",
       "               [ 0.35036355, -0.9366138 ,  6.748789  ],\n",
       "               [ 0.6251137 , -0.7805337 ,  6.3463287 ]],\n",
       " \n",
       "              [[-0.99808884,  0.06179533, -0.26376653],\n",
       "               [-0.9971635 ,  0.07526608, -0.270052  ],\n",
       "               [-0.9960703 ,  0.08856589, -0.26689482],\n",
       "               ...,\n",
       "               [-0.99320614, -0.11636806, -1.6603087 ],\n",
       "               [-0.9996702 , -0.02568067, -1.818975  ],\n",
       "               [-0.99754125,  0.07008158, -1.9164512 ]],\n",
       " \n",
       "              [[ 0.170985  ,  0.98527366, -0.15433198],\n",
       "               [ 0.13987711,  0.99016887,  0.6298392 ],\n",
       "               [ 0.06929094,  0.9975965 ,  1.4198172 ],\n",
       "               ...,\n",
       "               [ 0.80022275, -0.59970295,  6.1192584 ],\n",
       "               [ 0.94119287, -0.33786973,  5.969481  ],\n",
       "               [ 0.9990318 , -0.04399451,  6.0128145 ]]], dtype=float32),\n",
       " DeviceArray([[[ -0.05231199],\n",
       "               [ -0.03723671],\n",
       "               [ -0.02830905],\n",
       "               ...,\n",
       "               [ -6.3804293 ],\n",
       "               [ -4.831439  ],\n",
       "               [ -3.6307158 ]],\n",
       " \n",
       "              [[ -0.4688479 ],\n",
       "               [ -0.5481928 ],\n",
       "               [ -0.688088  ],\n",
       "               ...,\n",
       "               [-13.225301  ],\n",
       "               [-15.475228  ],\n",
       "               [-14.646116  ]],\n",
       " \n",
       "              [[ -4.7367764 ],\n",
       "               [ -4.682618  ],\n",
       "               [ -4.8202085 ],\n",
       "               ...,\n",
       "               [-11.129757  ],\n",
       "               [ -9.092232  ],\n",
       "               [ -7.2734857 ]],\n",
       " \n",
       "              ...,\n",
       " \n",
       "              [[ -3.5791972 ],\n",
       "               [ -3.6727998 ],\n",
       "               [ -4.000617  ],\n",
       "               ...,\n",
       "               [ -9.459357  ],\n",
       "               [ -7.5893908 ],\n",
       "               [ -6.02959   ]],\n",
       " \n",
       "              [[ -9.57261   ],\n",
       "               [ -9.491989  ],\n",
       "               [ -9.409341  ],\n",
       "               ...,\n",
       "               [ -8.864923  ],\n",
       "               [ -9.426277  ],\n",
       "               [-10.04003   ]],\n",
       " \n",
       "              [[ -2.0645504 ],\n",
       "               [ -1.9595814 ],\n",
       "               [ -2.085983  ],\n",
       "               ...,\n",
       "               [ -5.037882  ],\n",
       "               [ -4.1621494 ],\n",
       "               [ -3.6861699 ]]], dtype=float32),\n",
       " DeviceArray([[False, False, False, ..., False, False, False],\n",
       "              [False, False, False, ..., False, False, False],\n",
       "              [False, False, False, ..., False, False, False],\n",
       "              ...,\n",
       "              [False, False, False, ..., False, False, False],\n",
       "              [False, False, False, ..., False, False, False],\n",
       "              [False, False, False, ..., False, False, False]], dtype=bool),\n",
       " DeviceArray([-1076.0647, -1299.151 , -1502.7104, -1210.023 , -1287.3069,\n",
       "              -1267.0559, -1529.9363, -1517.0802, -1809.3643, -1565.379 ],            dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnax.experimental.rollout import EnvRollout\n",
    "\n",
    "roller = EnvRollout(model_forward=network.apply,\n",
    "                    env_name=\"Pendulum-v1\",\n",
    "                    num_env_steps=200,\n",
    "                    num_episodes=10)\n",
    "\n",
    "roller.collect(rng, policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Rollouts via `jax.vmap`/`jax.pmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.vmap across random keys for batch rollout\n",
    "batch_rollout = jax.vmap(jit_rollout, in_axes=(0, None, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.vmap across network params for \"population\" rollouts\n",
    "pop_rollout = jax.vmap(batch_rollout, in_axes=(None, vmap_dict, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.vmap across network params for meta-batch rollouts\n",
    "meta_rollout = jax.vmap(jit_rollout, in_axes=(None, None, vmap_dict, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Anakin Agent\n",
    "\n",
    "\n",
    "Adapted from Hessel et al. (2021) and DeepMind's [Example Colab](https://colab.research.google.com/drive/1974D-qP17fd5mLxy6QZv-ic4yxlPJp-G?usp=sharing#scrollTo=lhnJkrYLOvcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import chex\n",
    "import os\n",
    "os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n",
    "\n",
    "import jax\n",
    "import haiku as hk\n",
    "from jax import lax\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import rlax\n",
    "import timeit\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import `gymnax` and make `Catch-bsuite` environment transition/reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnax\n",
    "rng, env = gymnax.make(\"Catch-bsuite\")\n",
    "obs, state = env.reset(rng)\n",
    "action = env.action_space.sample(rng)\n",
    "obs, state, reward, terminal, info = env.step(rng, state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anakin Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TimeStep:\n",
    "    q_values: chex.Array\n",
    "    action: chex.Array\n",
    "    discount: chex.Array\n",
    "    reward: chex.Array\n",
    "\n",
    "def get_network_fn(num_outputs: int):\n",
    "    \"\"\"Define a fully connected multi-layer haiku network.\"\"\"\n",
    "    def network_fn(obs: chex.Array) -> chex.Array:\n",
    "        return hk.Sequential([  # flatten, hidden layer, relu, output layer.\n",
    "            hk.Flatten(), hk.Linear(256), jax.nn.relu, hk.Linear(num_outputs)])(obs)\n",
    "    return hk.without_apply_rng(hk.transform(network_fn))\n",
    "\n",
    "def get_learner_fn(\n",
    "    env, forward_pass, opt_update, rollout_len, agent_discount,\n",
    "    lambda_, iterations):\n",
    "    \"\"\"Define the minimal unit of computation in Anakin.\"\"\"\n",
    "\n",
    "    def loss_fn(params, outer_rng, env_state):\n",
    "        \"\"\"Compute the loss on a single trajectory.\"\"\"\n",
    "\n",
    "        def step_fn(env_state, rng):\n",
    "            obs = env.get_obs(env_state)\n",
    "            q_values = forward_pass(params, obs[None,])[0]  # forward pass.\n",
    "            action = jnp.argmax(q_values)  # greedy policy.\n",
    "            obs, state, reward, terminal, info = env.step(rng, env_state, action)  # step environment.\n",
    "            return env_state, TimeStep(  # return env state and transition data.\n",
    "              q_values=q_values, action=action, discount=1.-terminal, reward=reward)\n",
    "\n",
    "        step_rngs = random.split(outer_rng, rollout_len)\n",
    "        env_state, rollout = lax.scan(step_fn, env_state, step_rngs)  # trajectory.\n",
    "        qa_tm1 = rlax.batched_index(rollout.q_values[:-1], rollout.action[:-1])\n",
    "        td_error = rlax.td_lambda(  # compute multi-step temporal diff error.\n",
    "            v_tm1=qa_tm1,  # predictions.\n",
    "            r_t=rollout.reward[1:],  # rewards.\n",
    "            discount_t=agent_discount * rollout.discount[1:],  # discount.\n",
    "            v_t=jnp.max(rollout.q_values[1:], axis=-1),  # bootstrap values.\n",
    "            lambda_=lambda_)  # mixing hyper-parameter lambda.\n",
    "        return jnp.mean(td_error**2), env_state\n",
    "\n",
    "    def update_fn(params, opt_state, rng, env_state):\n",
    "        \"\"\"Compute a gradient update from a single trajectory.\"\"\"\n",
    "        rng, loss_rng = random.split(rng)\n",
    "        grads, new_env_state = jax.grad(  # compute gradient on a single trajectory.\n",
    "            loss_fn, has_aux=True)(params, loss_rng, env_state)\n",
    "        grads = lax.pmean(grads, axis_name='j')  # reduce mean across cores.\n",
    "        grads = lax.pmean(grads, axis_name='i')  # reduce mean across batch.\n",
    "        updates, new_opt_state = opt_update(grads, opt_state)  # transform grads.\n",
    "        new_params = optax.apply_updates(params, updates)  # update parameters.\n",
    "        return new_params, new_opt_state, rng, new_env_state\n",
    "\n",
    "    def learner_fn(params, opt_state, rngs, env_states):\n",
    "        \"\"\"Vectorise and repeat the update.\"\"\"\n",
    "        batched_update_fn = jax.vmap(update_fn, axis_name='j')  # vectorize across batch.\n",
    "        def iterate_fn(_, val):  # repeat many times to avoid going back to Python.\n",
    "            params, opt_state, rngs, env_states = val\n",
    "            return batched_update_fn(params, opt_state, rngs, env_states)\n",
    "        return lax.fori_loop(0, iterations, iterate_fn, (\n",
    "            params, opt_state, rngs, env_states))\n",
    "\n",
    "    return learner_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout/Step the Anakin Agent in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeIt():\n",
    "    def __init__(self, tag, frames=None):\n",
    "        self.tag = tag\n",
    "        self.frames = frames\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed_secs = timeit.default_timer() - self.start\n",
    "        msg = self.tag + (': Elapsed time=%.2fs' % self.elapsed_secs)\n",
    "        if self.frames:\n",
    "            msg += ', FPS=%.2e' % (self.frames / self.elapsed_secs)\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def run_experiment(env, batch_size, rollout_len, step_size, iterations, seed):\n",
    "    \"\"\"Runs experiment.\"\"\"\n",
    "    cores_count = len(jax.devices())  # get available TPU cores.\n",
    "    network = get_network_fn(env.action_space.num_categories)  # define network.\n",
    "    optim = optax.adam(step_size)  # define optimiser.\n",
    "\n",
    "    rng, rng_e, rng_p = random.split(random.PRNGKey(seed), num=3)  # prng keys.\n",
    "    obs, state = env.reset(rng_e)\n",
    "    dummy_obs = obs[None,]  # dummy for net init.\n",
    "    params = network.init(rng_p, dummy_obs)  # initialise params.\n",
    "    opt_state = optim.init(params)  # initialise optimiser stats.\n",
    "\n",
    "    learn = get_learner_fn(  # get batched iterated update.\n",
    "      env, network.apply, optim.update, rollout_len=rollout_len,\n",
    "      agent_discount=1, lambda_=0.99, iterations=iterations)\n",
    "    learn = jax.pmap(learn, axis_name='i')  # replicate over multiple cores.\n",
    "\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (cores_count, batch_size) + x.shape)\n",
    "    params = jax.tree_map(broadcast, params)  # broadcast to cores and batch.\n",
    "    opt_state = jax.tree_map(broadcast, opt_state)  # broadcast to cores and batch\n",
    "\n",
    "    rng, *env_rngs = jax.random.split(rng, cores_count * batch_size + 1)\n",
    "    env_obs, env_states = jax.vmap(env.reset)(jnp.stack(env_rngs))  # init envs.\n",
    "    rng, *step_rngs = jax.random.split(rng, cores_count * batch_size + 1)\n",
    "\n",
    "    reshape = lambda x: x.reshape((cores_count, batch_size) + x.shape[1:])\n",
    "    step_rngs = reshape(jnp.stack(step_rngs))  # add dimension to pmap over.\n",
    "    env_obs = reshape(env_obs)  # add dimension to pmap over.\n",
    "    env_states = {k: reshape(env_states[k]) for k in env_states.keys()}\n",
    "\n",
    "    with TimeIt(tag='COMPILATION'):\n",
    "        learn(params, opt_state, step_rngs, env_states)  # compiles\n",
    "\n",
    "    num_frames = cores_count * iterations * rollout_len * batch_size\n",
    "    with TimeIt(tag='EXECUTION', frames=num_frames):\n",
    "        params, opt_state, step_rngs, env_states = learn(  # runs compiled fn\n",
    "            params, opt_state, step_rngs, env_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running on', len(jax.devices()), 'cores.', flush=True)  # !expected 8!\n",
    "run_experiment(env, 128, 16, 1e-4, 100, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized Population Evaluation for CMA-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snippets",
   "language": "python",
   "name": "snippets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
